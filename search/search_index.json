{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AWS Cloud Tutorial","text":"<ul> <li> <p>Code Commit</p> <ul> <li>Clone Repo using SSH</li> <li>Push local repo to codecommit</li> <li>Change repository URL</li> </ul> </li> <li> <p>ECR</p> <ul> <li>Push Image to ECR Repository</li> </ul> </li> <li> <p>API Gateway</p> <ul> <li>Custom Domain for API Gateway</li> </ul> </li> <li> <p>Host Static Website in S3</p> </li> <li> <p>AWS Cognito</p> <ul> <li>AWS Cognito 3rd party SMS integration using lambda trigger</li> <li>Delete all cognito user poll users</li> </ul> </li> <li> <p>EC2</p> <ul> <li>Connect to EC2</li> <li>Connect to EC2 without key pair</li> <li>Increase Disk Space [EBS]</li> <li>Assign domain name/address to the EC2 instance</li> </ul> </li> <li> <p>Static IP to AWS Lambda function</p> </li> <li> <p>Neptune</p> <ul> <li>Connect to Neptune DB from local system</li> </ul> </li> </ul>"},{"location":"Cognito/General_Concepts/","title":"General Concepts","text":""},{"location":"Cognito/General_Concepts/#what-is-authentication-and-authorization","title":"What is Authentication and Authorization?","text":"<p><code>Authentication</code> - \"Who you are?\", is the process of ascertaining that somebody really is who they claim to be.</p> <p><code>Authorization</code> refers to rules that determine who is allowed to do what. E.g. Thirumal may be authorized to create and delete databases, while Jesicca is only authorized to read.</p>"},{"location":"Cognito/General_Concepts/#there-are-many-ways-of-authentication-few-of-which-are-worth-discussing-here","title":"There are many ways of authentication, few of which are worth discussing here:","text":"<ol> <li> <p><code>Knowledge-based authentication</code>: The username password combination is a type of knowledge-based authentication. The idea is to verify the user based on the knowledge of the user for example answer to security questions, passwords, something which only the user should know.</p> </li> <li> <p><code>Possession based authentication</code>: This type of authentication is based on verifying something which a user possesses. For example, when an application sends you One Time Passwords (OTPs) or a text message.</p> </li> </ol> <p>Modern authentication practices use a combination of both types, also known as <code>Multi-Factor authentication</code>.</p>"},{"location":"Cognito/General_Concepts/#password-handling-in-databases","title":"Password handling in databases","text":"<ol> <li>Storing it in the plain <code>String</code> format<ul> <li>Hacking methods<ul> <li>It's open to developers, database administrator, etc..</li> </ul> </li> </ul> </li> <li>Storing password as <code>HASH</code></li> <li>Possible attacks/hacking methods         * MD5/SHA1 collisions         * Rainbow Tables         * Dictionary attacks, brute-force(GPUs can compute billions of hashes/sec)</li> <li> <p>Salted HASH</p> <ul> <li>Possible attacks/hacking methods<ul> <li>Incorporate app-specific salt + random user-specific salt</li> <li>Use algorithm with configurable # of iteration (e.g. bycrypt, PBKDF2) to slow down brute force attacks</li> </ul> </li> </ul> </li> <li> <p>Secure Remote password (SRP) Protocol</p> <ul> <li>Verified based Protocol</li> <li>Password never travel over the wire</li> <li>Resistant to several attack vectors</li> <li>Perfect forward secrecy</li> </ul> </li> </ol>"},{"location":"Cognito/Intro/","title":"Intro","text":""},{"location":"Cognito/Intro/#introduction-to-aws-cognitio","title":"Introduction to AWS Cognitio","text":"<p>There are 2 types of pools in AWS Cognito</p> <ul> <li>User pools</li> <li>Identity pools</li> </ul> <p>The below picture clarifies both definition and difference</p> <p></p>"},{"location":"Cognito/RBAC/","title":"RBAC","text":""},{"location":"Cognito/RBAC/#rbac-role-based-access-control","title":"RBAC - Role Based Access Control","text":""},{"location":"Cognito/delete_all_user_from_user_pool/","title":"Delete all user from user pool","text":""},{"location":"Cognito/delete_all_user_from_user_pool/#delete-all-user-from-the-cognito-user-pool","title":"Delete all user from the Cognito user pool","text":""},{"location":"Cognito/delete_all_user_from_user_pool/#dependency","title":"Dependency","text":"<ol> <li> <p><code>aws cli</code> &amp; set up <code>IAM</code> config in home directory</p> </li> <li> <p>Install JQ</p> </li> </ol>"},{"location":"Cognito/delete_all_user_from_user_pool/#command-to-delete-all-users","title":"Command to delete all users","text":"<p>Replace the variable <code>COGNITO_USER_POOL_ID</code>  with <code>Pool Id</code> in the following command</p> <pre><code>aws cognito-idp list-users --user-pool-id $COGNITO_USER_POOL_ID | jq -r '.Users | .[] | .Username' | while read uname1; do echo \"Deleting $uname1\"; aws cognito-idp admin-delete-user --user-pool-id $COGNITO_USER_POOL_ID --username $uname1; done\n</code></pre>"},{"location":"Cognito/multi-tenancy/","title":"Multi tenancy","text":""},{"location":"Cognito/multi-tenancy/#multi-tenancy","title":"Multi Tenancy","text":"<ol> <li> <p>Tenant-specific Cognito configuration    Required if you want your customers to set up SAML/AD integration or configure themselves the Cognito password policies.</p> </li> <li> <p>Cognito Hosted UI    Required if you have a web application and don\u2019t want to maintain your own OAuth server (for details). You won\u2019t need this for mobile-only applications.</p> </li> <li> <p>Cross-tenant login page    Required if you can\u2019t ask your users to select the tenant e.g. by going to a tenant-specific URL like tenant.example.com</p> </li> <li> <p>Same email in multiple tenants    Required if you have users who need accounts in multiple of the tenants. For example: an implementation partner who supports multiple of your customers.</p> </li> </ol> <p>You might be able to work around this by asking your partners to use different email addresses per tenant, e.g. joe+tenant1@gmail.com, joe+tenant2@gmail.com, etc</p>"},{"location":"EC2/Increase%20Disk%20Space/","title":"Increase Disk Space","text":""},{"location":"EC2/Increase%20Disk%20Space/#increase-volumedisk-space-ebs","title":"Increase Volume/Disk space (EBS)","text":"<ul> <li>Select the <code>Instance</code>, you want to modify, and select <code>volume</code> under <code>storage</code>.</li> <li> <p>In the volume page select, select the volume and click on <code>Action</code> -&gt; <code>Modify Volume</code> -&gt; <code>Enter the volume size</code></p> </li> <li> <p>Connect to the EC2/Cloud 9 Instance</p> </li> <li>To verify the file system for each volume, use the <code>df -hT</code> command. </li> </ul> <pre><code>thirumal:~/environment $ df -hT\nFilesystem     Type      Size  Used Avail Use% Mounted on\nudev           devtmpfs  476M     0  476M   0% /dev\ntmpfs          tmpfs      98M  724K   98M   1% /run\n/dev/xvda1     ext4       9.7G  9.5G 128M  99% /\ntmpfs          tmpfs     490M     0  490M   0% /dev/shm\ntmpfs          tmpfs     5.0M     0  5.0M   0% /run/lock\ntmpfs          tmpfs     490M     0  490M   0% /sys/fs/cgroup\n/dev/loop0     squashfs   56M   56M     0 100% /snap/core18/1997\n/dev/loop2     squashfs   13M   13M     0 100% /snap/amazon-ssm-agent/495\n/dev/loop1     squashfs   34M   34M     0 100% /snap/amazon-ssm-agent/3552\n/dev/loop3     squashfs   88M   88M     0 100% /snap/core/5328\n/dev/loop4     squashfs  100M  100M     0 100% /snap/core/10958\ntmpfs          tmpfs      98M     0   98M   0% /run/user/1000\n</code></pre> <ul> <li>To check whether the volume has a partition that must be extended, use the lsblk command to display information </li> </ul> <p><pre><code>thirumal:~/environment $ lsblk\nNAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nloop0     7:0    0 55.5M  1 loop /snap/core18/1997\nloop1     7:1    0 33.3M  1 loop /snap/amazon-ssm-agent/3552\nloop2     7:2    0 12.7M  1 loop /snap/amazon-ssm-agent/495\nloop3     7:3    0 87.9M  1 loop /snap/core/5328\nloop4     7:4    0 99.2M  1 loop /snap/core/10958\nxvda    202:0    0   10G  0 disk \n\u2514\u2500xvda1 202:1    0   10G  0 part /\n</code></pre> * The above output shows, <code>xvda</code> has one partition * Extend/Grow the partition</p> <pre><code>sudo growpart /dev/xvda 1\n\n(OR)\nsudo growpart /dev/nvme0n1 1\n</code></pre> <p>or </p> <pre><code># resize filesystem\nthirumal.mari:~/environment $ sudo resize2fs /dev/xvda1\n</code></pre> <ul> <li>Check after resizing</li> </ul> <pre><code>thirumal.mari:~/environment $ df -hT\nFilesystem     Type      Size  Used Avail Use% Mounted on\nudev           devtmpfs  476M     0  476M   0% /dev\ntmpfs          tmpfs      98M  724K   98M   1% /run\n/dev/xvda1     ext4       49G  9.5G   39G  20% /\ntmpfs          tmpfs     490M     0  490M   0% /dev/shm\ntmpfs          tmpfs     5.0M     0  5.0M   0% /run/lock\ntmpfs          tmpfs     490M     0  490M   0% /sys/fs/cgroup\n/dev/loop0     squashfs   56M   56M     0 100% /snap/core18/1997\n/dev/loop2     squashfs   13M   13M     0 100% /snap/amazon-ssm-agent/495\n/dev/loop1     squashfs   34M   34M     0 100% /snap/amazon-ssm-agent/3552\n/dev/loop3     squashfs   88M   88M     0 100% /snap/core/5328\n/dev/loop4     squashfs  100M  100M     0 100% /snap/core/10958\ntmpfs          tmpfs      98M     0   98M   0% /run/user/1000\n</code></pre> <ul> <li>Extend the file system.</li> </ul> <pre><code>df -hT\n\nsudo xfs_growfs -d /\n\nsudo resize2fs /dev/nvme0n1p1\n</code></pre>"},{"location":"IAM/Policy%20for%20access%20specific%20folder/","title":"Policy for access specific folder","text":"<p><code>IAM policy</code> to access(Read/Write) specific folder in <code>S3</code></p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowStatement1\",\n            \"Action\": [\n                \"s3:ListAllMyBuckets\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::*\"\n            ]\n        },\n        {\n            \"Sid\": \"AllowStatement2B\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::BUCKET-NAME\"\n            ],\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"s3:prefix\": [\n                        \"\",\n                        \"sub-1\"\n                    ],\n                    \"s3:delimiter\": [\n                        \"/\"\n                    ]\n                }\n            }\n        },\n        {\n            \"Sid\": \"AllowStatement3\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::BUCKET-NAME\"\n            ],\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"FOLDER-NAME/*\"\n                    ]\n                }\n            }\n        },\n        {\n            \"Sid\": \"AllowStatement4B\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::BUCKET-NAME/FOLDER-NAME/*\"\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"S3/AWS-Cli%20in%20Ubuntu%20%26%20Mac/","title":"Install AWS CLi","text":"<pre><code>    sudo apt install awscli\n</code></pre>"},{"location":"S3/AWS-Cli%20in%20Ubuntu%20%26%20Mac/#set-up-user-config-in-ubuntumac-os","title":"Set Up User config in Ubuntu/Mac OS","text":"<ol> <li>Create IAM user &amp; have it ready <code>Access and Secrete key</code></li> <li> <p>Create <code>.aws</code> folder in your home directory</p> <pre><code>cd /home/thirumal\n\nmkdir .aws\n</code></pre> </li> <li> <p>Create <code>credentials</code> file in <code>.aws</code> folder</p> <pre><code>vi credentials\n</code></pre> </li> <li> <p>Add the user name inside <code>[]</code> and mention default user as <code>[default]</code>.</p> <pre><code>[default]\naws_access_key_id=\naws_secret_access_key=\nregion=ca-central-1\n[Thirumal]\naws_access_key_id=\naws_secret_access_key=\nregion=us-east-1\n[Jessica]\naws_access_key_id=\naws_secret_access_key=\nregion=ap-south-1\n</code></pre> </li> <li> <p>Usage of multiple profile.</p> <pre><code>aws syc s3://{bucketname}/{folderName}\n</code></pre> </li> </ol>"},{"location":"S3/Storage%20Class%20Transition/","title":"Lifecycle Rule","text":"<ol> <li>Select the <code>bucket</code> and go to <code>Management</code>.</li> <li>Click on <code>Create lifecycle rule</code></li> <li>Fill the details</li> <li>In the <code>Lifecycle rule actions</code>,</li> <li> <p>Move b/w class for versioned/non-current version select <code>Move noncurrent versions of objects between storage classes</code></p> </li> <li> <p>In <code>Transition noncurrent versions of objects between storage classes</code>, choose the storage class and select the no.of days to take to move to class <code>Days after objects become noncurrent</code></p> </li> </ol> <p> </p>"},{"location":"S3/commands/","title":"Commands","text":""},{"location":"S3/commands/#to-set-up-aws-cli-check-set-up-aws-cli-in-ubuntumac","title":"To set up aws cli check Set up aws cli in Ubuntu/Mac","text":""},{"location":"S3/commands/#list-buckets","title":"List buckets","text":"<pre><code>aws s3 ls\n</code></pre>"},{"location":"S3/commands/#copy-local-file-from-local-to-s3","title":"Copy Local file from <code>Local</code> to <code>S3</code>","text":"<pre><code>aws s3 cp {fileName} s3://{bucketname}/{folderName}\n</code></pre>"},{"location":"S3/commands/#copy-multiple-files-from-local-system-to-aws-s3-bucket","title":"Copy multiple files from <code>Local</code> System to <code>AWS S3</code> bucket","text":"<pre><code>aws s3 cp {loacl_path} s3://{bucketName}/{folderName} --recursive\n</code></pre>"},{"location":"S3/commands/#download-object-from-s3-to-local-system","title":"Download object from <code>S3</code> to local system","text":"<pre><code>aws s3 cp s3://{bucketNamw}/{folderName} /home/thirumal/Downloads\n</code></pre> <p>--Download multiple files</p> <pre><code>aws s3 cp s3://{bucketName}/{folderName} /home/thirumal/Downloads/ --recursive\n</code></pre>"},{"location":"S3/commands/#use-double-quotes-if-there-is-any-space-in-directory-file-name","title":"Use double quotes, if there is any space in directory / file name","text":"<pre><code> aws s3 cp {fileName} \"s3://{bucketname}/{folderName}\"\n</code></pre>"},{"location":"S3/cross_account_S3_copy/","title":"Copy S3 objects from one account to another account S3 bucket","text":"<ol> <li>Create the buckets or get the bucket names from source and destination account.</li> <li> <p>Attach the following policy to source bucket.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DelegateS3Access\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::DESTINATION_BUCKET_ACCOUNT_NUMBER:user/thirumal\"\n            },\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::SOURCE_BUCKET_NAME/*\",\n                \"arn:aws:s3:::SOURCE_BUCKET_NAME\"\n            ]\n        }\n    ]\n}\n</code></pre> </li> <li> <p>Create <code>IAM</code> user in the destination account with the following <code>policy</code>.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::SOURCE_BUCKET_NAME\",\n                \"arn:aws:s3:::SOURCE_BUCKET_NAME/*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:PutObject\",\n                \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::DESTINATION_BUCKET_NAME\",\n                \"arn:aws:s3:::DESTINATION_BUCKET_NAME/*\"\n            ]\n        }\n    ]\n}\n</code></pre> </li> <li> <p>Run the following command with destination <code>IAM</code> user in the local machine.</p> <pre><code>aws sync s3://sourceBucket/folder_name s3://destinationBucket/folder_name\n</code></pre> </li> </ol>"},{"location":"S3-Host%20static%20website/host_static_website_using_s3/","title":"Host static website using S3","text":"<ol> <li> <p>Create two S3 bucket with your domain name <code>(www and non-www)</code></p> <ol> <li>example.com  </li> <li>www.example.com</li> </ol> </li> <li> <p>Configure <code>non-www</code> bucket to redirect to <code>www</code> bucket</p> </li> </ol> <p></p> <ol> <li> <p>Upload your web content to www bucket</p> </li> <li> <p>Configure www bucket for web-hosting by selecting <code>static website hosting</code></p> </li> </ol> <p></p> <ol> <li>Make www bucket as <code>public</code> and add the below <code>policy</code></li> </ol> <p><pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicReadGetObject\",\n            \"Effect\":\"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::www.example.com/*\"\n        }\n    ]\n}\n</code></pre> </p> <p></p> <ol> <li> <p>Create AWS route53 public hosted zone with the same name as the domain name (Non-WWW).</p> </li> <li> <p>Update GoDaddy NS with AWS provided NS (without ending dot(.).</p> </li> </ol> <p></p> <ol> <li> <p>In AWS Route53 </p> <p>create <code>A Type</code>  recordset for the <code>non-www</code> domain and select </p> <ol> <li>Enable <code>Alias</code></li> <li>Alias to S3 website endpoint, </li> <li>Select the region of your bucket</li> <li>Select your <code>S3 main bucket</code> (i.e non-www domain)</li> </ol> <p>create <code>A Type</code> recordset for <code>www</code> subdomain and repeat the above steps/points except the selecting S3 <code>www</code> bucket.</p> </li> </ol> <p></p> <ol> <li>Access your web site from browser. Enjoy</li> </ol>"},{"location":"codecommit/Push%20local%20repo%20to%20codecommit/","title":"Push local repo to codecommit","text":""},{"location":"codecommit/Push%20local%20repo%20to%20codecommit/#pushset-up-local-repository-to-aws-code-commit","title":"Push/Set up Local Repository to AWS Code Commit","text":""},{"location":"codecommit/Push%20local%20repo%20to%20codecommit/#instructions","title":"Instructions","text":"<p>Initialize your sample app as a git repository:</p> <pre><code>git init\n</code></pre> <p>Commit your existing files:</p> <pre><code>git add -A .\ngit commit -m \"Initial commit\"\n</code></pre> <p>Copy the value for the SourceRepoURL and configure a new git remote named codecommit. Be sure to use your value of the SourceRepoURL:</p> <pre><code>git remote add origin https://git-codecommit.us-west-2.amazonaws.com/v1/repos/mytodo\n\n(OR)\n\ngit remote add codecommit https://git-codecommit.us-west-2.amazonaws.com/v1/repos/mytodo\n</code></pre> <p>Configure the CodeCommit credential helper. Append these lines to the end of your .git/config file:</p> <pre><code>[credential]\n    helper =\n    helper = !aws codecommit credential-helper $@\n    UseHttpPath = true\n</code></pre>"},{"location":"codecommit/Push%20local%20repo%20to%20codecommit/#verification","title":"Verification","text":"<p>Verify you have a codecommit remote:</p> <pre><code>git remote -v\ncodecommit  https://git-codecommit.us-west-2.amazonaws.com/v1/repos/mytodo (fetch)\ncodecommit  https://git-codecommit.us-west-2.amazonaws.com/v1/repos/mytodo (push)\n</code></pre> <p>Verify the credential helper is installed correctly. Mac users may see an osxkeychain entry as the first line of output. This is expected, you just need to verify the last two lines match the output below:</p> <pre><code>git config -l | grep helper\ncredential.helper=osxkeychain\ncredential.helper=\ncredential.helper=!aws codecommit credential-helper $@\n</code></pre> <p>Verify you can fetch from the codecommit remote:</p> <pre><code>git fetch codecommit\n\necho $?\n\n0\n</code></pre>"},{"location":"codecommit/Push%20local%20repo%20to%20codecommit/#pushing-your-changes-to-aws-codecommit","title":"Pushing your changes to AWS CodeCommit","text":"<p>Now we have our pipeline and git remote configured, anytime we push changes to our codecommit remote, our pipeline will automatically deploy our app. Instructions</p> <p>Push your changes to the codecommit remote:</p> <pre><code>git push codecommit master\n\nCounting objects: 23, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (18/18), done.\nWriting objects: 100% (23/23), 9.82 KiB | 3.27 MiB/s, done.\nTotal 23 (delta 2), reused 0 (delta 0)\nTo https://git-codecommit.us-west-2.amazonaws.com/v1/repos/mytodo\n    * [new branch]      master -&gt; master\n</code></pre>"},{"location":"codecommit/Push%20local%20repo%20to%20codecommit/#verification_1","title":"Verification","text":"<p>The best way to verify the pipeline is working as expected is to view the pipeline in the console:</p>"},{"location":"codecommit/change_repo_url/","title":"Change repo url","text":""},{"location":"codecommit/change_repo_url/#change-codecommit-repository-url","title":"Change codecommit Repository URL","text":"<ol> <li>Copy url from the codecommit</li> <li>Delete remote url <pre><code>git remote set-url --delete codecommit {old_URL}\n</code></pre> you may get the following error but ignore it. <pre><code>fatal: Will not delete all non-push URLs\n</code></pre></li> <li>Add new remote url <pre><code>git remote set-url --add codecommit {new_URL}\n</code></pre></li> <li>Check the URL <pre><code>git remote -v\n</code></pre></li> <li>Repeat the setp 2 <pre><code>git remote set-url --delete codecommit {old_URL}\n</code></pre></li> </ol>"},{"location":"codecommit/clone%20using%20ssh/","title":"Clone using ssh","text":""},{"location":"codecommit/clone%20using%20ssh/#step-1-set-up-your-default-identity","title":"Step 1. Set up your default identity","text":"<ol> <li>Open Terminal (Ctrl + Alt + T) and enter the following command to set up your default identity</li> </ol> <pre><code>ssh-keygen\n</code></pre> <p>Eg:</p> <pre><code>thirumal@thirumal:~/git$ ssh-keygen\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/thirumal/.ssh/id_rsa): \nPress enter to accept the default key and path, `/c/Users/&lt;username&gt;/.ssh/id_rsa`.\n Enter and re-enter a passphrase when prompted.The command creates your default identity with its public and private keys. The whole interaction looks similar to this:\n\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /home/thirumal/.ssh/id_rsa\nYour public key has been saved in /home/thirumal/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:h4xKiyvxzOWkeXcwxmkAYSZ9AsDOikB14+vgzpTnjPs thirumal@thirumal\nThe keys randomart image is:\n+---[RSA 3072]----+\n|*o=o o           |\n| *+ + .          |\n|+  + .           |\n|.o  . .o .       |\n|+  ..+..S .      |\n|+ .o*oB  .       |\n| =.Xo= o         |\n|. X.B . .        |\n| ..*oE .         |\n+----[SHA256]-----+\n</code></pre>"},{"location":"codecommit/clone%20using%20ssh/#step-2-add-the-public-key-to-your-account-settings","title":"Step 2 Add the public key to your Account settings","text":"<ul> <li>Copy your public key using the following command </li> </ul> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> <ul> <li> <p>Add/Upload it to your <code>IAM</code> -&gt; <code>security credentials</code></p> </li> <li> <p>Add the generated AWS <code>SSH key ID</code> to <code>~.ssh/config</code></p> </li> </ul> <pre><code>  Host git-codecommit.*.amazonaws.com\n    User {SSHKEYID}\n    IdentityFile ~/.ssh/id_rsa\n</code></pre> <p>or </p> <pre><code>  Host git-codecommit.*.amazonaws.com\n    User {SSHKEYID}\n    IdentityFile ~/.ssh/id_rsa\n    PubkeyAcceptedAlgorithms +ssh-rsa\n    HostkeyAlgorithms +ssh-rsa\n</code></pre> <ul> <li>Return to your terminal and clone the repository using</li> </ul> <pre><code>git clone {SSH_URL}\n</code></pre>"},{"location":"codepipeline/ReactJs_host_in_S3/","title":"ReactJs host in S3","text":"<ol> <li> <p>Create a repository in AWS codecommit or Github</p> </li> <li> <p>Create a bucket to host website, follow the instructions S3-Host static website</p> </li> <li> <p>Go to AWS CODEPIPELINE, Create New</p> <ul> <li>Source - Select the repository and branch</li> <li>Build  - Create a new project with <code>buildspec.yml</code> options<ul> <li>Create <code>buildspec.yml</code> file in the reactjs repository with the following contents<pre><code>version: 0.2\n\nphases:\n  pre_build:\n    commands:\n      - npm install\n  build:\n    commands:\n      - npm run build\n\nartifacts:\n  files:\n    - '**/*'\n  discard-paths: no\n  base-directory: build\n* Deploy - Select the bucket you want to deploy.\n</code></pre> </li> </ul> </li> </ul> </li> </ol>"},{"location":"codepipeline/host_plain_html_css_ins3/","title":"Host plain html css ins3","text":"<ol> <li>Create a repository in AWS codecommit or Github</li> <li>Add <code>buildspec.yml</code> file in root directory </li> <li> <p>Create a bucket to host website, follow the instructions S3-Host static website</p> </li> <li> <p>Go to AWS CODEPIPELINE, Create New</p> <ul> <li>Source - Select the repository and branch</li> <li> <p>Build  - Create a new project with <code>buildspec.yml</code> options</p> <ul> <li>Create <code>buildspec.yml</code> file in the reactjs repository with the following contents<pre><code>version: 0.2\nartifacts:\n  files:\n    - '**/*'\n  discard-paths: no\n</code></pre> </li> </ul> </li> <li> <p>Deploy - Select the bucket you want to deploy.</p> </li> </ul> </li> </ol>"},{"location":"lambda/trigger_anatomy/","title":"Trigger anatomy","text":""},{"location":"lambda/trigger_anatomy/#anatomy-of-aws-lambda-function","title":"Anatomy of AWS Lambda function","text":"<pre><code>import json\n\ndef lambda_handler(event, context):\n    return {\n        'statusCode': 200,\n        'body': json.dumps('Hello World')\n    }\n</code></pre>"},{"location":"lambda/SAM/pycharm-sam-lambda/","title":"Pycharm sam lambda","text":""},{"location":"lambda/SAM/pycharm-sam-lambda/#set-up-lambda-function-in-pycharm-using-aws-samserverless-application-model","title":"Set up lambda function in pycharm using AWS SAM(Serverless Application Model)","text":"<ol> <li> <p>Install PyCharm with AWS toolkit &amp; SAM plugin, AWS CLI &amp; configure IAM user.</p> </li> <li> <p>New Project -&gt; AWS Serverless Application -&gt; Give project name in the location and choose the runtime and hit the create button</p> </li> </ol> <p></p> <ol> <li>Wait for some time to set up all config.</li> </ol>"},{"location":"lambda/SAM/pycharm-sam-lambda/#run-locally","title":"Run Locally","text":"<ol> <li> <p>Install and start <code>DOCKER</code></p> </li> <li> <p>Go to pycharm -&gt; <code>Run</code> -&gt; <code>Edit configurations</code> -&gt; select input text <code>hello world</code> template in the configuration.</p> <p></p> <p></p> </li> <li> <p>Check the <code>Build function inside a container</code> in <code>SAMCLI</code>.</p> </li> <li> <p>Click on <code>Run Locally</code></p> </li> </ol>"},{"location":"lambda/SAM/pycharm-sam-lambda/#deploy","title":"Deploy","text":"<ol> <li> <p>Right click on project and click on <code>Deploy Serverless Application</code>.</p> </li> <li> <p>It requires permission on</p> <ul> <li>AWS Cloud formation</li> <li>S3</li> <li>lambda</li> <li>API Gateway</li> <li>IAM</li> </ul> </li> </ol>"},{"location":"lambda/SAM/pycharm-sam-lambda/#tutorial","title":"Tutorial","text":"<p>https://www.jetbrains.com/pycharm/guide/tutorials/intro-aws/cleanup/</p>"},{"location":"lambda/SAM/pycharm-sam-lambda/#troubleshoot-know-problem","title":"Troubleshoot Know Problem","text":"<ol> <li> <p>Docker permission</p> <p>Preferences -&gt; Resources -&gt; File Sharing -&gt; Add \"/Application\".</p> </li> </ol>"},{"location":"lambda/SAM/sam-template/","title":"SAM Template","text":""},{"location":"lambda/SAM/sam-template/#add-environment-variables","title":"Add Environment variables","text":"<pre><code>Resources:\n  HelloWorldFunction:\n    Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction\n    Properties:\n      CodeUri: eballot_presignup/\n      Handler: app.lambda_handler\n      Runtime: python3.8\n      Environment:\n        Variables:\n          APP_TABLE_NAME: user\n</code></pre>"},{"location":"lambda/SAM/sam-template/#permissionpolicy-for-lambda-function","title":"Permission/Policy for lambda Function","text":"<pre><code>Resources:\n  HelloWorldFunction:\n    Type: AWS::Serverless::Function # More info about Function Resource: https://github.com/awslabs/serverless-application-model/blob/master/versions/2016-10-31.md#awsserverlessfunction\n    Properties:\n      CodeUri: eballot_presignup/\n      Handler: app.lambda_handler\n      Runtime: python3.8\n      Environment:\n        Variables:\n          APP_TABLE_NAME: user\n      Policies:\n        # Give DynamoDB Full Access to your Lambda Function\n        - AmazonDynamoDBFullAccess\n</code></pre>"},{"location":"lambda/chalice/CI_CD_trigger/","title":"CI/CD for lambda trigger fucntions","text":"<p>Annotate the <code>lambda_handler(event, context)</code> with <code>@app.lambda_function()</code></p> <pre><code>@app.lambda_function()\ndef lambda_handler(event, context):\n    logging.info('Event %s', event)\n    logging.info('Context %s', context)\n    return event;\n</code></pre>"},{"location":"lambda/chalice/Creating%20a%20pipeline/","title":"Creating a pipeline","text":"<p>AWS Chalice provides a command for generating a starter template. This template is managed through an AWS CloudFormation stack. Instructions</p> <p>Create a release/ directory. We\u2019ll place CD related files in this directory:</p> <pre><code>$ mkdir release/\n\nGenerate a CloudFormation template for our starter CD pipeline:\n\n$ chalice generate-pipeline release/pipeline.json\n\nDeploy this template using the AWS CLI:\n\n$ aws cloudformation deploy --stack-name chalice-pipeline-stack \\\n    --template-file release/pipeline.json \\\n    --capabilities CAPABILITY_IAM\n</code></pre> <p>This last command may take up a few minutes to deploy. Configuring git</p> <p>Up to this point, we have not been using any source control to track our changes to our sample app. We\u2019re now going to create and configure a git repo along with an AWS CodeCommit remote. If you haven\u2019t set up git, you can follow the instructions in the Setting up git section. Instructions</p> <pre><code>Initialize your sample app as a git repository:\n\n$ git init .\n$ cp ../chalice-workshop/code/todo-app/part2/02-pipeline/.gitignore .\n</code></pre> <p>Commit your existing files:</p> <pre><code>$ git add -A .\n$ git commit -m \"Initial commit\"\n</code></pre> <p>Query the CloudFormation stack you created in the previous step for the value of the remote repository:</p> <pre><code>$ aws cloudformation describe-stacks \\\n    --stack-name chalice-pipeline-stack \\\n    --query 'Stacks[0].Outputs'\n[\n    ...\n    {\n        \"OutputKey\": \"SourceRepoURL\",\n        \"OutputValue\": \"https://git-codecommit.us-west-2.amazonaws.com/v1/repos/mytodo\"\n    },\n    ...\n]\n</code></pre> <p>Copy the value for the SourceRepoURL and configure a new git remote named codecommit. Be sure to use your value of the SourceRepoURL:</p> <pre><code>$ git remote add codecommit https://git-codecommit.us-west-2.amazonaws.com/v1/repos/mytodo\n</code></pre> <p>Configure the CodeCommit credential helper. Append these lines to the end of your .git/config file:</p> <pre><code>[credential]\n    helper =\n    helper = !aws codecommit credential-helper $@\n    UseHttpPath = true\n</code></pre>"},{"location":"lambda/chalice/Creating%20a%20pipeline/#verification","title":"Verification","text":"<p>Verify you have a codecommit remote:</p> <pre><code>$ git remote -v\ncodecommit  https://git-codecommit.us-west-2.amazonaws.com/v1/repos/mytodo (fetch)\ncodecommit  https://git-codecommit.us-west-2.amazonaws.com/v1/repos/mytodo (push)\n</code></pre> <p>Verify the credential helper is installed correctly. Mac users may see an osxkeychain entry as the first line of output. This is expected, you just need to verify the last two lines match the output below:</p> <pre><code>$ git config -l | grep helper\ncredential.helper=osxkeychain\ncredential.helper=\ncredential.helper=!aws codecommit credential-helper $@\n</code></pre> <p>Verify you can fetch from the codecommit remote:</p> <pre><code>$ git fetch codecommit\n$ echo $?\n0\n</code></pre>"},{"location":"lambda/chalice/Creating%20a%20pipeline/#pushing-your-changes-to-aws-codecommit","title":"Pushing your changes to AWS CodeCommit","text":"<p>Now we have our pipeline and git remote configured, anytime we push changes to our codecommit remote, our pipeline will automatically deploy our app. Instructions</p> <p>Push your changes to the codecommit remote:</p> <pre><code>$ git push codecommit master\nCounting objects: 23, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (18/18), done.\nWriting objects: 100% (23/23), 9.82 KiB | 3.27 MiB/s, done.\nTotal 23 (delta 2), reused 0 (delta 0)\nTo https://git-codecommit.us-west-2.amazonaws.com/v1/repos/mytodo\n * [new branch]      master -&gt; master\n</code></pre>"},{"location":"lambda/chalice/Creating%20a%20pipeline/#verification_1","title":"Verification","text":"<p>The best way to verify the pipeline is working as expected is to view the pipeline in the console:</p> <pre><code>Log in to the AWS Console at https://console.aws.amazon.com/console/home\n\nGo to the CodePipeline page.\n\nClick on the \u201cmytodoPipeline\u201d pipeline.\n</code></pre> <p>../../_images/pipeline-landing.png</p> <pre><code>You should see a \u201cSource\u201d, \u201cBuild\u201d, and \u201cBeta\u201d stage.\n\nIt can take a few minutes after pushing a change before the pipeline starts. If your pipeline has not started yet, wait a few minutes and refresh the page. Once the pipeline starts, it will take about 10 minutes for the intial deploy.\n</code></pre> <p>../../_images/pipeline-started.png</p> <p>Wait until the stages have completed and all the stages are green. ../../_images/pipeline-finished.png</p> <pre><code>Place your mouse over the \u201ci\u201d icon. Note the value of the Stack name. It should be something like mytodoBetaStack.\n</code></pre> <p>../../_images/pipeline-stack-name.png</p> <pre><code>Query for the stack output of EndpointURL using the AWS CLI. This is the same step we performed in the previous section:\n\n$ aws cloudformation describe-stacks --stack-name mytodoBetaStack \\\n    --query 'Stacks[0].Outputs'\n[\n    {\n        \"OutputKey\": \"APIHandlerArn\",\n        \"OutputValue\": \"arn:aws:lambda:us-west-2:123:function:...\"\n    },\n    {\n        \"OutputKey\": \"APIHandlerName\",\n        \"OutputValue\": \"...\"\n    },\n    {\n        \"OutputKey\": \"RestAPIId\",\n        \"OutputValue\": \"abcd\"\n    },\n    {\n        \"OutputKey\": \"EndpointURL\",\n        \"OutputValue\": \"https://your-chalice-url/api/\"\n    }\n]\n\nUse the value for EndpointURL to test your API by creating a new Todo item:\n\n$ echo '{\"description\": \"My third Todo\", \"metadata\": {}}' | \\\n    http POST https://your-chalice-url/api/todos\nHTTP/1.1 200 OK\nContent-Length: 36\nContent-Type: application/json\n\nabcdefg-abcdefg\n\nVerify you can retrieve this item:\n\n$ http https://your-chalice-url/todos/abcdefg-abcdefg\nHTTP/1.1 200 OK\nContent-Length: 140\nContent-Type: application/json\n\n{\n    \"description\": \"My third Todo\",\n    \"metadata\": {},\n    \"state\": \"unstarted\",\n    \"uid\": \"abcdefg-abcdefg\",\n    \"username\": \"default\"\n}\n</code></pre>"},{"location":"lambda/chalice/Creating%20a%20pipeline/#deploying-an-update","title":"Deploying an update","text":"<p>Now we\u2019ll make a change to our app and commit/push our change to CodeCommit. Our change will automatically be deployed. Instructions</p> <pre><code>At the bottom of your app.py file, add a new test route:\n\n@app.route('/test-pipeline')\ndef test_pipeline():\n    return {'pipeline': 'route'}\n\nCommit and push your changes:\n\n$ git add app.py\n$ git commit -m \"Add test view\"\n$ git push codecommit master\nCounting objects: 3, done.\nDelta compression using up to 4 threads.\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 357 bytes | 357.00 KiB/s, done.\nTotal 3 (delta 2), reused 0 (delta 0)\nTo https://git-codecommit.us-west-2.amazonaws.com/v1/repos/mytodo\n   4ded202..31f2dc3  master -&gt; master\n</code></pre> <p>Verification</p> <pre><code>Go back to the AWS Console page for your CodePipeline named \u201cmytodoPipeline\u201d.\n\nRefresh the page. You should see the pipeline starting again. If you\u2019re not seeing any changes, you may need to wait a few minutes and refresh.\n\nWait for the pipeline to finish deploying.\n\nOnce it\u2019s finished verify the new test route is available. Use the same EndpointURL from the previous step:\n\n$ http https://your-chalice-url/api/test-pipeline\nHTTP/1.1 200 OK\nConnection: keep-alive\nContent-Length: 21\nContent-Type: application/json\n...\n\n{\n    \"pipeline\": \"route\"\n}\n</code></pre> <p>Extract the buildspec to a file</p> <p>The instructions for how CodeBuild should package our app lives in the release/pipeline.json CloudFormation template. CodeBuild also supports loading the build instructions from a buildspec.yml file at the top level directory of your app. In this step we\u2019re going to extract out the build spec from the inline definition of the release/pipeline.json into a buildspec.yml file. This will allow us to modify how CodeBuild should build our app without having to redeploy our pipeline stack. Instructions</p> <pre><code>Remove the BuildSpec key from your release/pipeline.json file. Your existing template has this section:\n\n\"Resources\": {\n  \"AppPackageBuild\": {\n    \"Type\": \"AWS::CodeBuild::Project\",\n      \"Source\": {\n        \"BuildSpec\": \" ... long string here ...\",\n        \"Type\": \"CODEPIPELINE\"\n      }\n    }\n...\n\nAnd after removing the BuildSpec key it should look like this:\n\n\"Resources\": {\n  \"AppPackageBuild\": {\n    \"Type\": \"AWS::CodeBuild::Project\",\n      \"Source\": {\n        \"Type\": \"CODEPIPELINE\"\n      }\n    }\n...\n\nRedeploying your pipeline stack using the AWS CLI:\n\n$ aws cloudformation deploy --stack-name chalice-pipeline-stack \\\n    --template-file release/pipeline.json \\\n    --capabilities CAPABILITY_IAM\n\nAt the top level directory of your sample app, create a new file named buildspec.yml with these contents:\n\nversion: 0.1\nphases:\n  install:\n    commands:\n      - sudo pip install --upgrade awscli\n      - aws --version\n      - sudo pip install chalice\n      - sudo pip install -r requirements.txt\n      - chalice package /tmp/packaged\n      - aws cloudformation package --template-file /tmp/packaged/sam.json --s3-bucket ${APP_S3_BUCKET} --output-template-file transformed.yaml\nartifacts:\n  type: zip\n  files:\n    - transformed.yaml\n\nCommit the buildspec.yml file and push your changes to CodeCommit:\n\n$ git add buildspec.yml\n$ git commit -m \"Adding buildspec.yml\"\n$ git push codecommit master\n</code></pre> <p>Verification</p> <pre><code>Go to the CodePipeline page in the console.\n\nWait for the pipeline to deploy your latest changes. Keep in mind that there should be no functional changes, we just want to verify that CodeBuild was able to load the buildspec.yml file.\n</code></pre> <p>Run unit tests</p> <p>Now we\u2019re going to modify our buildspec.yml file to run our unit tests. If the tests fail our application won\u2019t deploy to our Beta stage. Instructions</p> <pre><code>Create a new build.sh script with these contents:\n\n#!/bin/bash\npip install --upgrade awscli\naws --version\npip install virtualenv\nvirtualenv /tmp/venv\n. /tmp/venv/bin/activate\npip install -r requirements.txt\npip install -r requirements-test.txt\npip install chalice\nexport PYTHONPATH=.\npy.test tests/ || exit 1\nchalice package /tmp/packaged\naws cloudformation package --template-file /tmp/packaged/sam.json --s3-bucket \"${APP_S3_BUCKET}\" --output-template-file transformed.yaml\n\nMake the script executable:\n\n$ chmod +x ./build.sh\n\nUpdate your buildspec.yml to call this build script:\n\nversion: 0.1\nphases:\n  install:\n    commands:\n      - sudo -E ./build.sh\nartifacts:\n  type: zip\n  files:\n    - transformed.yaml\n\nCommit your changes and push them to codecommit:\n\n$ git add build.sh buildspec.yml\n$ git commit -m \"Run unit tests\"\n</code></pre> <p>Verification</p> <pre><code>Refresh your pipeline in the AWS console.\n\nVerify the pipeline successfully completes.\n</code></pre> <p>Add a failing test</p> <p>Now we\u2019ll add a failing unit test and verify that our application does not deploy. Instructions</p> <pre><code>Add a failing test to the end of the tests/test_db.py file:\n\ndef test_fail():\n    assert 0 == 1\n\nCommit and push your changes:\n\n$ git add tests/test_db.py\n$ git commit -m \"Add failing test\"\n$ git push codecommit master\n</code></pre> <p>Verification</p> <pre><code>Refresh your pipeline in the AWS console.\n\nVerify that the CodeBuild stage fails.\n</code></pre>"},{"location":"lambda/chalice/Policy/","title":"Policy","text":""},{"location":"lambda/chalice/Policy/#addmodify-the-default-policy","title":"Add/Modify the default policy","text":"<ol> <li> <p>Create project will automatically create .chalice/config.json file</p> <pre><code>{\n  \"version\": \"2.0\",\n  \"app_name\": \"eballot-add-signup-user\",\n  \"stages\": {\n    \"dev\": {\n       \"api_gateway_stage\": \"api\"\n    }\n  }\n}\n</code></pre> </li> <li> <p>To add policy for each environment, add the policy json file to the same directory</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Action\": [\n        \"logs:CreateLogGroup\",\n        \"logs:CreateLogStream\",\n        \"logs:PutLogEvents\"\n      ],\n      \"Resource\": \"arn:aws:logs:*:*:*\",\n      \"Effect\": \"Allow\"\n    },\n    {\n      \"Sid\": \"VisualEditor0\",\n      \"Effect\": \"Allow\",\n      \"Action\": \"kinesis:*\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre> </li> <li> <p>Modify the <code>config.json</code> with policy file name and stage</p> <pre><code>{\n  \"version\": \"2.0\",\n  \"app_name\": \"eballot-add-signup-user\",\n  \"stages\": {\n    \"dev\": {\n       \"autogen_policy\": false,\n       \"iam_policy_file\": \"policy-dev.json\",\n       \"api_gateway_stage\": \"api\"\n    }\n  }\n}\n</code></pre> </li> </ol>"},{"location":"lambda/chalice/blueprint/","title":"Blueprint","text":""},{"location":"lambda/chalice/blueprint/#chalice-blueprints-are-used-to-organize-your-application-into-logical-components","title":"Chalice blueprints are used to organize your application into logical components","text":"<ol> <li> <p>Create <code>chalicelib</code> folder with the logical components</p> <pre><code>mkdir chalicelib\ntouch chalicelib/__init__.py\ntouch chalicelib/blueprints.py\n</code></pre> </li> <li> <p>Configure the <code>decoration</code> in <code>chalicelib/blueprints.py</code> file:</p> <pre><code>from chalice import Blueprint\n\nextra_routes = Blueprint(__name__)\n\n@extra_routes.route('/foo')\ndef foo():\n    return {'foo': 'bar'}\n</code></pre> <p>The <code>__name__</code> is used to denote the import path of the blueprint. This name must match the import name of the module so the function can be properly imported when running in Lambda. We\u2019ll now import this module in our app.py and register this blueprint. We\u2019ll also add a route in our app.py directly:</p> <ol> <li> <p>Import in <code>app.py</code></p> <pre><code>from chalice import Chalice\nfrom chalicelib.blueprints import extra_routes\n\napp = Chalice(app_name='blueprint-demo')\napp.register_blueprint(extra_routes)\n\n\n@app.route('/')\ndef index():\n    return {'hello': 'world'}\n</code></pre> </li> </ol> </li> </ol>"},{"location":"lambda/chalice/setup-chalice/","title":"Setup chalice","text":""},{"location":"lambda/chalice/setup-chalice/#aws-chalice","title":"AWS Chalice","text":""},{"location":"lambda/chalice/setup-chalice/#installation","title":"Installation","text":"<p>To install Chalice, we\u2019ll first create and activate a virtual environment in python3.7:</p> <pre><code>$ python3 --version\nPython 3.7.3\n$ python3 -m venv venv37\n$ . venv37/bin/activate\n</code></pre> <p>Next we\u2019ll install Chalice using pip: <pre><code>$ python3 -m pip install chalice\n</code></pre> You can verify you have chalice installed by running: <pre><code>$ chalice --help\nUsage: chalice [OPTIONS] COMMAND [ARGS]...\n...\n</code></pre></p>"},{"location":"lambda/chalice/setup-chalice/#create-new-chalice-project","title":"Create new Chalice Project","text":"<p>The next thing we\u2019ll do is use the chalice command to create a new project: <pre><code>$ chalice new-project helloworld\n</code></pre></p>"},{"location":"lambda/chalice/setup-chalice/#deploying","title":"Deploying\u00b6","text":"<pre><code>Let\u2019s deploy this app. Make sure you\u2019re in the helloworld directory and run chalice deploy:\n</code></pre> <p><code>$ chalice deploy</code></p>"},{"location":"lambda/chalice/test/","title":"Test Chalice","text":""},{"location":"lambda/chalice/test/#requirementdependencies","title":"Requirement/Dependencies","text":"<pre><code>pip install pytest\n</code></pre>"},{"location":"lambda/chalice/test/#files","title":"Files","text":"<p>create a new <code>tests/</code> directory and create a <code>tests/__init__.py</code> and a <code>tests/test_app.py</code> file.</p> <pre><code>mkdir tests\ntouch tests/{__init__.py,test_app.py}\n</code></pre>"},{"location":"lambda/chalice/test/#run","title":"Run","text":"<pre><code>py.test tests/test_app.py\n</code></pre>"},{"location":"lambda/chalice/test/#run-test-with-printlogs-using-flag-s","title":"Run test with print/logs using flag <code>-s</code>","text":"<pre><code>py.test tests/test_app.py -s\n</code></pre>"},{"location":"lambda/chalice/test/#run-test-with-log-statement","title":"Run test with log statement","text":"<pre><code>pytest --log-cli-level=DEBUG\n</code></pre>"},{"location":"serverless-framework/Chalice/","title":"Chalice","text":""},{"location":"serverless-framework/Chalice/#aws-chalice-application-debug","title":"AWS Chalice Application Debug","text":"<p>AWS Chalice is a Python micro-framework, designed to provide developers with a familiar flask-like experience when developing serverless applications.</p>"},{"location":"serverless-framework/Chalice/#debug-locally","title":"Debug locally","text":"<pre><code>chalice local\n</code></pre>"},{"location":"serverless-framework/Chalice/#debug-with-pycham","title":"Debug with PyCham","text":"<pre><code>chalice local --no-autoreload\n</code></pre> <p>In PyCharm, go to the <code>Run</code> menu and choose the option <code>Attach to Process</code>\u2026.</p> <p>A new menu will open, the menu lists all python processes. Select the chalice local process to attach a debugger to it.</p>"},{"location":"serverless-framework/Chalice/#troubleshoot","title":"Troubleshoot","text":"<ol> <li> <p>Deploy <code>timeout</code></p> <p><code>chalice deploy --connection-timeout 120</code></p> </li> <li> <p>Error <code>botocore.exceptions.ClientError: An error occurred (InvalidArgument) when calling the PutBucketNotificationConfiguration</code></p> <p>Solution: Make sure <code>S3</code> bucket location <code>Lambda</code> location are same.</p> </li> <li> <p>Custom API gateway</p> <p>Delete <code>.chalice/deployed</code> and create new. Make sure API gateway is updated in AWS Console</p> </li> </ol>"},{"location":"serverless-framework/chalice%20multi%20stage%20deploy/","title":"Chalice multi-stage deployment","text":"<ol> <li>Add stages config on <code>.chalice/config.json</code> file.  </li> <li>Example: <code>dev</code> and <code>prod</code> stages <pre><code>  {\n    \"version\": \"2.0\",\n    \"app_name\": \"test-stage\",\n    \"stages\": {\n      \"dev\": {\n        \"autogen_policy\": false,\n        \"iam_policy_file\": \"policy.json\",\n        \"api_gateway_stage\": \"api\",\n        \"environment_variables\": {\n          \"env1\": \"dev-values\"\n        },\n        \"subnet_ids\": [\"subnet-dev\"],\n        \"security_group_ids\": [\"sg-dev\"]\n      },\n      \"prod\": {\n        \"autogen_policy\": false,\n        \"iam_policy_file\": \"policy.json\",\n        \"api_gateway_stage\": \"api\",\n        \"environment_variables\": {\n          \"env1\": \"pro-values\"\n        },\n        \"subnet_ids\": [\"subnet-prod\"],\n        \"security_group_ids\": [\"sg-prod\"]\n      }\n    }\n  }\n</code></pre></li> <li> <p>To deploy, run the following commands</p> <p>chalice deploy --stage prod    chalice deploy --stage dev</p> </li> </ol>"},{"location":"ses/How%20to%20get%20out%20of%20Sandbox%20and%20increase%20AWS%20sending%20limits/","title":"How to get out of Sandbox and increase AWS sending limits","text":"<ol> <li>In the AWS Management Console, click on the \u201cSupport menu.\u201d</li> <li>Now select \u201cSupport Center\u201d from the drop-down menu. </li> <li>You are now in the \u201cAWS Support Center\u201d page. Click on the \u201cCreate case\u201d button.</li> <li>On the \u201cCreate case\u201d section, select \u201cService limit increase.\u201d</li> <li>Then a panel named \u201cCase classification\u201d will show up. Select \u201cSES Sending Limits\u201d from the dropdown menu under \u201cLimit Type.\u201d</li> <li>Choose the type of email that you want to send for \u201cMail Type.\u201d For example, we selected \u201cMarketing\u201d here. If there are more than one value applies, choose the option that applies to most of the emails that you want to send.</li> <li>Next, \u201cWebsite URL\u201d-  type the URL of your business website. Providing the website\u2019s URL will help AWS better understand the content you are sending. For example, here we have entered our website address:</li> <li>In \u201cHow you will only send to recipients who have specifically requested your mail\u201d \u2013 ??</li> <li> <p>We send emails only to the registered users and to those who have subscribed to our messages.</p> </li> <li> <p>In \u201cThe process that you will follow when you receive bounce and complaint notifications\u201d  </p> </li> <li>The software we will be using to send emails through amazonSES will handle the bounce and complaint for us. </li> <li>For bounce: If the bounce is permanent, the software unsubscribes the lead immediately. And no email will be sent in future to that address. If the bounce is not permanent, and the application just stores the response related to the bounce (like timestamp, type, subtype) for future use. </li> <li> <p>For complaint: The application unsubscribes the email address immediately. And no email will be sent in future to that address.</p> </li> <li> <p>In \u201cWill you will comply with AWS Service Terms and AUP,\u201d select \u201cYES.\u201d </p> </li> <li>Under \u201cRequests\u201d for \u201cRegion\u201d select the AWS Region in which you verified your email earlier and have configured the application to use. For example, we\u2019ve chosen \u2013 \u201cAP-South 1 (Mumbai)\u201d here.</li> <li> <p>After selecting the \u201cRegion\u201d you will see \u201cLimit\u201d choose the type of quota increment that you need to request. There are two options:     \u2022 Desired Daily Send Quota: You can select this option if you want to request an increment in the number of emails that can be sent from your account per 24-hour period in the specified region.     \u2022 Desired Maximum Send Rate: Select this option if you want to request for an increment to the number of emails your account will be able to send each second in that specified region.</p> </li> <li> <p>For \u201cNew limit value\u201d enter the limit that you want to be increased. Request the amount that you think you\u2019ll be in need of. Keep in mind, it is not ensured that you will receive the amount you require. For a new user, we suggest entering 50000, but you can enter a higher number according to your needs.</p> <ol> <li>Note: You can request a \u201csending rate increment\u201d at the same time by clicking the \u201cAdd Another Request\u201d button. Since Amazon increases the sending rate automatically in accordance with the sending quota, we are skipping this now for simplicity.</li> </ol> </li> <li> <p>In the \u201cCase Description\u201d section, for \u201cUse case description\u201d explain how you will use Amazon SES to send emails.</p> </li> <li>Under \u201cContact options\u201d for \u201cPreferred contact language\u201d select the language you want Amazon to communicate with you. For example, we have selected \u201cEnglish\u201d here.</li> <li>When you are done, click \u201cSubmit.\u201d</li> </ol>"},{"location":"tls/create_cloud_front/","title":"Set up Cloud Front for S3 static web site","text":""},{"location":"tls/create_cloud_front/#steps","title":"Steps","text":"<ol> <li> <p>Generate TLS &amp; add the certificate to <code>Certificate Manager</code>. Refer Let's Encrypt doc</p> </li> <li> <p>Go to Cloud Front and select <code>Create distribution</code> and again select <code>Get Started</code> under <code>Web</code>.</p> </li> <li> <p>Copy the end point from <code>www domain name</code> S3 bucket (Caution: Don't select the auto-select option) and add to <code>Origin Domain name</code></p> </li> </ol> <p></p> <ol> <li> <p>In <code>Default Cache Behavior Settings</code> -&gt; <code>Viewer Protocol policy</code>  select <code>Redirect HTTP to HTTPS</code></p> </li> <li> <p>In <code>Distribution Settings</code> add <code>Alternate Domain Names (CNAMEs)</code> </p> <p><code>www.example.com</code> 6. In <code>SSL Certificate</code> select <code>Custom SSL Certificate (example.com)</code> and select the <code>imported certificate</code> from the <code>Certificate Manager</code></p> </li> </ol> <p> </p> <ol> <li>Leave everything with default configuration &amp; create</li> </ol>"},{"location":"tls/create_cloud_front/#point-domain-to-cloudfront","title":"Point Domain to CloudFront","text":"<ol> <li>Copy the <code>Domain Name</code> that is displayed in the row after create</li> </ol> <ol> <li>Add that as <code>CNAME</code> in <code>Route53</code></li> </ol> <p>Expected error <code>Key is not found</code> will be in the origin path (Clear)</p>"},{"location":"vpc/Key%20component%20of%20VPC/","title":"Key component of VPC","text":""},{"location":"vpc/Key%20component%20of%20VPC/#key-component-of-vpc","title":"Key component of VPC","text":"<ol> <li> <p>Internet Gateway &amp; NAT: It logically enables routing of traffic in the public Network</p> </li> <li> <p>DNS: Standard which resolve names used over the internet in to IP address.</p> </li> <li> <p>Elastic IP: It's a static IP which never changes (supports only IPv4)</p> </li> <li> <p>VPC Endpoints: Private connection between VPC &amp; other AWS services without using internet</p> </li> <li> <p>Network Interface: A point of connection between public and private network.</p> </li> <li> <p>Egress only IG: Allow only outbound communication from EC2 over IPv6.</p> </li> <li> <p>Route Tables: Defines how traffic is routed between each subnet.</p> </li> <li> <p>VPC peering: Connection between VPCs.</p> </li> </ol>"},{"location":"vpc/subnet/","title":"Subnet","text":""},{"location":"vpc/subnet/#subnet","title":"Subnet","text":""},{"location":"vpc/subnet/#what-is-subnet","title":"What is subnet?","text":"<p>A subnet is a logical subdivision of large network.</p> <p>IP address with common prefix are in the same subnet</p> <p></p>"},{"location":"vpc/subnet/#types-of-subnet","title":"Types of subnet:","text":"<ol> <li> <p>private subnet</p> <ol> <li> <p>Resources are not exposed to the outside world</p> </li> <li> <p>They make use of only private IP</p> </li> <li> <p>Mainly used for backend application</p> </li> </ol> </li> <li> <p>public subnet</p> <ol> <li> <p>Resources are exposed to the internet through internet gateway</p> </li> <li> <p>They make use of both private  and public IPs</p> </li> <li> <p>Mainly used for external facing application UI/Web server</p> </li> </ol> </li> </ol>"},{"location":"vpc/vpc/","title":"Vpc","text":""},{"location":"vpc/vpc/#vpc-virtual-private-cloud","title":"VPC - Virtual Private cloud","text":"<ol> <li> <p>Logically isolated section of AWS Cloud.</p> </li> <li> <p>Can be applied to all the region/zone</p> </li> <li> <p>Subnet, NAT, security group, internet gateway, etc are the part of VPC</p> </li> </ol>"},{"location":"vpc/vpc/#why-its-required","title":"Why it's required?","text":"<ol> <li> <p>privacy</p> </li> <li> <p>Security</p> </li> <li> <p>Prevention from loss of proprietary data</p> </li> </ol>"},{"location":"vpc/vpc/#what-is-an-ip-address","title":"What is an IP address?","text":"<p>An IP address is a logical, numerical label assigned as a unique entity to each device in a network to locate the host/system through the network ID &amp; Host ID present in the IP address</p>"},{"location":"vpc/vpc/#what-is-cidr","title":"What is CIDR?","text":"<p>Classless Inter-Domain routing reduces the wastage of IP addresses by extra required number of IP address to the users. It represents the IP address along with special number (/n) to specify the number of bits present in the network ID.</p> <pre><code>172.19.0.0/16\n\n1010 1100 001 111/ 0000 0000 0000  0000\n\nNetwork ID      /  Host ID\n</code></pre>"}]}